---
title: STAT3255 - Traveler's Case Competition
author: Jack Bienvenue
format: html
---

# Pre-cleaning Evaluation

Before we begin working with the Traveler's dataset, we will clean it.

It happens that many of the fields in the dataset are filled with some idiosyncratic conventions, such as missing values being entered as "Miss" in one of the dataset's columns.

Before touching the data though, let's begin by inspecting the present columns by printing samples from them and checking data types.

**Printout 1 - Data Sample & Data Types:**

*Printout 1.1 - Data Sample*
``` {python}
#| echo: False

# First, import packages:

import matplotlib.pyplot as plt
import pandas as pd
from prettytable import PrettyTable
import numpy as np

# Next, import data:

training_df = pd.read_csv("data/train_data.csv")
test_df = pd.read_csv("data/test_data.csv")

# Finally, print data head and dtypes:

## Print head

def print_dataframe_in_chunks(df, chunk_size=4): #Fxn for output niceness
    ## Ierate over the columns in chunks
    for start in range(0, len(df.columns), chunk_size):
        end = start + chunk_size
        
        table = PrettyTable() #create prettytable object
        
        table.field_names = df.columns[start:end].tolist()
        
        for row in df.head().itertuples(index=False):
            table.add_row(row[start:end])
        
        ## Print the table
        print(table)
        print()

print_dataframe_in_chunks(training_df)

```

*Printout 1.2 - Datatypes*
``` {python}
#| echo: False

## Print datatypes nicely
d_types = pd.DataFrame(training_df.dtypes, columns=['Data Type'])

table = PrettyTable() # Instantiate table 

table.field_names = ["Column Name", "Data Type"] 

## Add rows to the table
for column, dtype in d_types.itertuples(index=True):
    table.add_row([column, dtype])

## Printout
print(table)

```

From our outputs, we find that we are working with a variety of data types and formats. 

For two more checks before we start remedying problems, we need to know just a little more about the content of each column. This is important because from our printing, it looks as though some columns like can be "prdct_sbtyp_grp" can be converted to a binary variable, but there might actually be far more options than we know about. Our first check will be checking the unique values.

We also want to consider missing values for each column. We might find that some predictors may not be able to be used in practice for modeling, or that we may be able to fill in missing values.

Let's get into it:

**Printout 2 - Unique and Missing Values:**

*Printout 2.1 - Unique Values:*

Here, we will examine unique values based upon the combined training set and test set to make sure we don't accidentally process the data in a way where we filter out values that are absent in the training set but present in the test set. 

```{python}
#| echo: False

# We will construct a printout table in this section

## First, let's get the concatenated complete data:

complete_df = pd.concat([training_df, test_df])

## Define unique counts:

unique_counts = complete_df.nunique()

table = PrettyTable() # Instantiate table
table.field_names = ['Column Name', 'Unique Values Count']

## Add entries

for column in unique_counts.index:
    table.add_row([column, unique_counts[column]])

## Show
print(table)

```

Now, for the datasets with a small number of unique values, let's examine what those unique values are:

``` {python}
#| echo: false

## For print formatting:
    ## Abbreviate longest column names

complete_df2 = complete_df.rename(
    columns={'digital_contact_ind': 'dig_cont_ind',
             'has_prior_carrier': 'pri_carrier',
             'household_group': 'household_g',
             'prdct_sbtyp_grp': 'pr_sbtyp_g',
             'pol_edeliv_ind': 'edeliv_ind',
             'bi_limit_group': 'bi_lim_g',
             'telematics_ind': 'telm_ind',
             'pay_type_code': 'pay_typ_c',
             'product_sbtyp': 'prdct_sbtyp'
    }
)

unique_counts2 = complete_df2.nunique()

## Pick out columns with less than 10 unique values
columns_with_few_unique = unique_counts2[unique_counts2 < 10].index

table = PrettyTable() # Instantiate table
table.field_names = ["Column Name", "Unique Values"] # Set fields

## Add rows to the table for each qualifying column
for column in columns_with_few_unique:
    unique_values = complete_df2[column].unique()
    table.add_row([column, ', '.join(map(str, unique_values))])

## Print table
print(table)
```

*Printout 2.2 - Missing Values:*

Are there any missing values in the entire dataframe?:

``` {python}
#| echo: False

if training_df.isnull().values.any():
    print("Training Set: There are missing values in the DataFrame.")
else:
    print("Training Set: No missing values in the DataFrame.")

if test_df.isnull().values.any():
    print("Testing Set: There are missing values in the DataFrame.")
else:
    print("Testing Set: No missing values in the DataFrame.")
```

This is a good start, however if we stopped here, we'd be forgetting something. As mentioned on the [Kaggle data page for the competition](https://www.kaggle.com/competitions/2024-travelers-umc-u-conn/data), there are some individualized conventions for missing values in each column. 

Let's observe for which columns this is true:

``` {python}
#| echo: false

## Column names as list
column_names = complete_df.columns.tolist()

## Remove IDs for this exercise
s_column_names = column_names[1:]

## Arrange to sync to the special formatting:
ordered_columns = [
    'ann_prm_amt',
    'bi_limit_group',
    'channel',
    'newest_veh_age',
    'geo_group',
    'has_prior_carrier',
    'home_lot_sq_footage',
    'household_group',
    'household_policy_counts',
    'telematics_ind',
    'digital_contact_ind',
    '12m_call_history',
    'tenure_at_snapshot',
    'pay_type_code',
    'acq_method',
    'trm_len_mo',
    'pol_edeliv_ind',
    'prdct_sbtyp_grp',
    'product_sbtyp',
    'call_counts'
]

## Reorder the DataFrame
ordered_complete_df = complete_df[ordered_columns]

s_column_names_sorted = [col for col in ordered_columns if col in s_column_names]

special_missing_format = {
    'Column Name': s_column_names_sorted,
    'Special Format': ['No', 'No', 'No', 'Yes', 
                        'No', 'No', 'No', 'No',
                        'No', 'Yes', 'No', 'No',
                        'No', 'No', 'Yes', 'No',
                        'Yes', 'No', 'No', 'No'
                        ]
}

## Turn into df
special_df = pd.DataFrame(special_missing_format)

table = PrettyTable() # Instantiate table 

table.field_names = ["Column Name", "Special Format"] 

# Add rows to the table
for row in special_df.itertuples(index=True):
    table.add_row([row._1, row._2])

print(table)

```

# Data Cleaning

We now have a quite holistic view of the data we are working with. Now, we are able to start cleaning the columns which require cleaning.

``` {python} 
#| echo: False

# Here, we will clean both the training and testing set individually,
# and we will go on to export both individually.

def travelers_data_cleaning(df):

    # Step 1: Cleaning unusual conventions for missing values

    ## Clean the newest_veh_age column:

    df['newest_veh_age'] = df['newest_veh_age'].replace(-20, np.nan)

    ## Clean the telematics_ind column:

    df['telematics_ind'] = df['telematics_ind'].replace(0, np.nan)

        ### FIXME! How to treat '-2' non-auto types?

    ## Clean the acq_method column:

    df['acq_method'] = df['acq_method'].replace('Miss', np.nan)

    ## Clean the pol_edeliv_ind column:

    df['pol_edeliv_ind'] = df['pol_edeliv_ind'].replace(-2, np.nan)


    # Step 2: Making binary 

    # Step 3: Getting dummies for categorical variables:

    ## 


    # Step 4:

    return clean_df

```

Note that we will make the following changes to the dataset:

- acq_method
    - !!!!!!!!!!!!!!!!!!!!!
    -
- bi_limit_group
    - !!!!!!!!!!!!!!!!!!!!!
    - 
- channel
    - Convert to binary, with 'Retail' as 1, 'Others' as 0. 
- dig_cont_ind
    - Already binary, 1 if customer *has* opted into digital communications
- edeliv_ind
    - 
- geo_group
    - Create dummy variables with 'rural' as baseline
- household_g
    - !!!!!!!!!!!!!!!!!!!!!
    -
- newest_veh_age
    - Convert entries of -20 to NaN
- pay_typ_c
    - Create dummy variables with 'type1' as baseline
- pri_carrier
    - Already binary, 1 indicates that the customer came from prior carrier



``` {python}
#| echo: False

```