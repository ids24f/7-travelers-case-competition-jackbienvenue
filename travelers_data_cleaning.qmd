---
title: STAT3255 - Traveler's Case Competition
author: Jack Bienvenue
format: html
---

# Pre-cleaning Evaluation

Before we begin working with the Traveler's dataset, we will clean it.

It happens that many of the fields in the dataset are filled with some idiosyncratic conventions, such as missing values being entered as "-20" in one of the dataset's columns.

Before touching the data though, let's begin by inspecting the present columns by printing samples from them and checking data types.

**Printout 1 - Data Sample & Data Types:**

*Printout 1.1 - Data Sample*
``` {python}
#| echo: False

# First, import packages:

import matplotlib.pyplot as plt
import pandas as pd
from prettytable import PrettyTable
import numpy as np

# Next, import data:

training_df = pd.read_csv("data/train_data.csv")
test_df = pd.read_csv("data/test_data.csv")

# Finally, print data head and dtypes:

## Print head

def print_dataframe_in_chunks(df, chunk_size=4): #Fxn for output niceness
    ## Ierate over the columns in chunks
    for start in range(0, len(df.columns), chunk_size):
        end = start + chunk_size
        
        table = PrettyTable() #create prettytable object
        
        table.field_names = df.columns[start:end].tolist()
        
        for row in df.head().itertuples(index=False):
            table.add_row(row[start:end])
        
        ## Print the table
        print(table)
        print()

print_dataframe_in_chunks(training_df)

```

*Printout 1.2 - Datatypes*
``` {python}
#| echo: False

## Print datatypes nicely
d_types = pd.DataFrame(training_df.dtypes, columns=['Data Type'])

table = PrettyTable() # Instantiate table 

table.field_names = ["Column Name", "Data Type"] 

## Add rows to the table
for column, dtype in d_types.itertuples(index=True):
    table.add_row([column, dtype])

## Printout
print(table)

```

From our outputs, we find that we are working with a variety of data types and formats. 

For two more checks before we start remedying problems, we need to know just a little more about the content of each column. This is important because from our printing, it looks as though some columns like can be "prdct_sbtyp_grp" can be converted to a binary variable, but there might actually be far more options than we know about. Our first check will be checking the unique values.

We also want to consider missing values for each column. We might find that some predictors may not be able to be used in practice for modeling, or that we may be able to fill in missing values.

Let's get into it:

**Printout 2 - Unique and Missing Values:**

*Printout 2.1 - Unique Values:*

Here, we will examine unique values based upon the combined training set and test set to make sure we don't accidentally process the data in a way where we filter out values that are absent in the training set but present in the test set. 

```{python}
#| echo: False

# We will construct a printout table in this section

## First, let's get the concatenated complete data:

complete_df = pd.concat([training_df, test_df])

## Define unique counts:

unique_counts = complete_df.nunique()

table = PrettyTable() # Instantiate table
table.field_names = ['Column Name', 'Unique Values Count']

## Add entries

for column in unique_counts.index:
    table.add_row([column, unique_counts[column]])

## Show
print(table)

```

Note that there are 100,000 entries between the training set and test set, split into 80,000 and 20,000, respectively. The 'id' field only shows 80,000 unique values because in the test set, the IDs are given as numbers 1-20,000 instead of 80,001 through 100,000.

Now, for the datasets with a small number of unique values, let's examine what those unique values are:

``` {python}
#| echo: false

## For print formatting:
    ## Abbreviate longest column names

complete_df2 = complete_df.rename(
    columns={'digital_contact_ind': 'dig_cont_ind',
             'has_prior_carrier': 'pri_carrier',
             'household_group': 'household_g',
             'prdct_sbtyp_grp': 'pr_sbtyp_g',
             'pol_edeliv_ind': 'edeliv_ind',
             'bi_limit_group': 'bi_lim_g',
             'telematics_ind': 'telm_ind',
             'pay_type_code': 'pay_typ_c',
             'product_sbtyp': 'prdct_sbtyp'
    }
)

unique_counts2 = complete_df2.nunique()

## Pick out columns with less than 10 unique values
columns_with_few_unique = unique_counts2[unique_counts2 < 10].index

table = PrettyTable() # Instantiate table
table.field_names = ["Column Name", "Unique Values"] # Set fields

## Add rows to the table for each qualifying column
for column in columns_with_few_unique:
    unique_values = complete_df2[column].unique()
    table.add_row([column, ', '.join(map(str, unique_values))])

## Print table
print(table)
```

*Printout 2.2 - Missing Values:*

Are there any missing values in the entire dataframe?:

``` {python}
#| echo: False

if training_df.isnull().values.any():
    print("Training Set: There are missing values in the DataFrame.")
else:
    print("Training Set: No missing values in the DataFrame.")

if test_df.isnull().values.any():
    print("Testing Set: There are missing values in the DataFrame.")
else:
    print("Testing Set: No missing values in the DataFrame.")
```

This is a good start, however if we stopped here, we'd be forgetting something. As mentioned on the [Kaggle data page for the competition](https://www.kaggle.com/competitions/2024-travelers-umc-u-conn/data), there are some individualized conventions for missing values in each column. 

Let's observe for which columns this is true:

``` {python}
#| echo: false

## Column names as list
column_names = complete_df.columns.tolist()

## Remove IDs for this exercise
s_column_names = column_names[1:]

## Arrange to sync to the special formatting:
ordered_columns = [
    'ann_prm_amt',
    'bi_limit_group',
    'channel',
    'newest_veh_age',
    'geo_group',
    'has_prior_carrier',
    'home_lot_sq_footage',
    'household_group',
    'household_policy_counts',
    'telematics_ind',
    'digital_contact_ind',
    '12m_call_history',
    'tenure_at_snapshot',
    'pay_type_code',
    'acq_method',
    'trm_len_mo',
    'pol_edeliv_ind',
    'prdct_sbtyp_grp',
    'product_sbtyp',
    'call_counts'
]

## Reorder the DataFrame
ordered_complete_df = complete_df[ordered_columns]

s_column_names_sorted = [col for col in ordered_columns if col in s_column_names]

special_missing_format = {
    'Column Name': s_column_names_sorted,
    'Special Format': ['No', 'No', 'No', 'Yes', 
                        'No', 'No', 'No', 'No',
                        'No', 'Yes', 'No', 'No',
                        'No', 'No', 'Yes', 'No',
                        'Yes', 'No', 'No', 'No'
                        ]
}

## Turn into df
special_df = pd.DataFrame(special_missing_format)

table = PrettyTable() # Instantiate table 

table.field_names = ["Column Name", "Special Format"] 

# Add rows to the table
for row in special_df.itertuples(index=True):
    table.add_row([row._1, row._2])

print(table)

```

# Data Cleaning

We now have a quite holistic view of the data we are working with. Now, we are able to start cleaning the columns which require cleaning.

``` {python} 
#| echo: False

# Here, we will clean both the training and testing set individually,
# and we will go on to export both individually.

## First, a convenience function for dummy
## variable creation with specified reference
def create_dummies_with_reference(df, column, reference):

    ## Create dummy variables
    dummies = pd.get_dummies(df[column], prefix=column)
    
    ## Drop the reference category
    if f'{column}_{reference}' in dummies.columns:
        dummies = dummies.drop(f'{column}_{reference}', axis=1)
    
    ## Combine with the original DataFrame (excluding the original categorical column)
    df_final = pd.concat([df.drop(column, axis=1), dummies], axis=1)
    
    return df_final

## Now, we'll build our big cleaning function:

def travelers_data_cleaning(df):

    # Step 1: Cleaning unusual conventions for missing values

    ## Clean the newest_veh_age column:

    df['newest_veh_age'] = df['newest_veh_age'].replace(-20, np.nan)

    ## Clean the telematics_ind column:

    df['telematics_ind'] = df['telematics_ind'].replace(0, np.nan)

        ### FIXME! How to treat '-2' non-auto types?

    ## Clean the acq_method column:

    df['acq_method'] = df['acq_method'].replace('Miss', np.nan)

    ## Clean the pol_edeliv_ind column:

    df['pol_edeliv_ind'] = df['pol_edeliv_ind'].replace(-2, np.nan)
    df['pol_edeliv_ind'] = df['pol_edeliv_ind'].replace(-1, np.nan)


    # Step 2: Making qualifying fields binary

    ## Binary conversion of 'channel'
    df['channel'] = df['channel'].replace('Retail', 1)

    df['channel'] = df['channel'].replace('Others', 0)

    ## Binary converstion of trm_len_mo

    df['trm_len_mo'] = df['trm_len_mo'].replace('12', 0)

    df['trm_len_mo'] = df['trm_len_mo'].replace('6', 1)

    # Step 3: Getting dummies for categorical variables:

    ## dummies for acq_method

    df = create_dummies_with_reference(df, 'acq_method', 'method1')

    ## dummies for bi_limit_group

    data = {
    'bi_lim_g': ['NonAuto', 'SPGrp1Miss', 'SPGrp2', 'CSLGrp2', 
                 'CSLGrp1', 'SPGrp3', 'SPGrp4', 'CSLGrp3']
    }
    df = pd.DataFrame(data)

    ### Define a mapping for the values
    mapping = {
        'NonAuto': 'NonAuto',
        'SPGrp1Miss': 'SP',
        'SPGrp2': 'SP',
        'CSLGrp2': 'CSL',
        'CSLGrp1': 'CSL',
        'SPGrp3': 'SP',
        'SPGrp4': 'SP',
        'CSLGrp3': 'CSL'
    }

    ### Replace values in the column using the mapping
    df['bi_lim_g'] = df['bi_lim_g'].map(mapping)

    ### Now create dummy variables
    dummies = pd.get_dummies(df['bi_lim_g'], prefix='bi_lim_g')

    ### Combine with the original DataFrame
    df_final = pd.concat([df.drop('bi_lim_g', axis=1), dummies], axis=1)


    ## dummies for geo_group

    df = create_dummies_with_reference(df, 'geo_group', 'rural')

    ## household_group 

    df = create_dummies_with_reference(df, 'household_group', '1dwelling')

    ## pay_type_code

    df = create_dummies_with_reference(df, 'pay_type_code', 'type1')

    ## product_sbtyp

    df = create_dummies_with_reference(df, 'product_sbtyp', 'A')

    ## prdct_sbtyp_grp

    df = create_dummies_with_reference(df, 'prdct_sbtyp_grp', 'type1')

    # Step 4:

    return clean_df

```

Note that we will make the following changes to the dataset:

- **acq_method**
    - Create dummies with 'method1' serving as baseline
    - Replace 'missing' with NaN
        - NOTE: 20% of values are missing
- **bi_limit_group**
    - Create dummies for just NonAuto, SP, and CSL
- **channel**
    - Convert to binary, with 'Retail' as 1, 'Others' as 0. 
- **dig_cont_ind**
    - Already binary, 1 if customer *has* opted into digital communications
- **geo_group**
    - Create dummy variables with 'rural' as baseline
- **household_group**
    - Create dummies, '1dwelling' as baseline
- **newest_veh_age**
    - Convert entries of -20 to NaN
- **pay_type_code**
    - Create dummy variables with 'type1' as baseline
- **pol_edeliv_ind**
    - 1 indicates email delivery of documents, 0 is paper delivery.
    - **Replace missing indicator '-2' with NaN**
        - **Additionally, replace meaningless '1' with NaN**
- **product_sbtyp**
    - Create dummy variables with 'A' as baseline
- **prdct_sbtyp_grp**
    - Create dummy variables with 'type1' as baseline
- **pri_carrier**
    - Already binary, 1 indicates that the customer came from prior carrier
- **telematics_ind**
    - Replace existing missing value '0' with NaN
- **trm_len_mo**
    - Convert to binary, 12 month -> 0, 6 month -> 1

Now, before moving on to modeling, let's check on the count of missing values in the fields that may have them. 

``` {python}
#| echo: False

```