---
title: STAT3255 - Traveler's Case Competition - Modeling
author: Jack Bienvenue
format: html
---

``` {python}
#| echo: false

## Package import
import lightgbm as lgbm
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

## Import clean data
train_df = pd.read_csv('data/clean_training_data.csv')
test_df = pd.read_csv('data/clean_testing_data.csv')
```

Time for the fun part of this competition, that is, doing the modeling!

# Strategy

For any competition, it is important to go in with a strategy. Every competition has its rules, and we can use these to our advantage. On the [Kaggle page](https://www.kaggle.com/competitions/2024-travelers-umc-u-conn/overview), it is mentioned that the *baseline* model, our model to beat, is an untuned LightGBM model:

::: {.callout}
"**Benchmark Model:**

The benchmark will be LightGBM model without any tuning or feature manipulation. We will provide it before the first optional submission."
:::

Given that we know the type of baseline model and we know that it is *untuned*, this presents an opportunity for what should be a surefire beat of the benchmark model if we replicate the baseline method *with* tuning. 

We will try this, and in addition we will consider simpler models. Sometimes simpler models might help us achieve comparable or better results in a much simpler way. We will try out a GLM to do this because we know that the data is heavy-tailed with many entries of $0$ for the call count response.

# LightGBM with Tuning

``` {python}
#| echo: false

# Initial tuning of parameters, data prep

params = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'max_depth': -1,
}

## Set X_train, y_train, X_test, y_test

# NOTE: our *model* train and test set are both from
# just the Kaggle training set (test set does not have)
# entries for call_counts

X = train_df.drop(columns=['call_counts'])
y = train_df['call_counts']

## Split data (80/20)
X_train, X_test, y_train, y_test = (
    train_test_split(X, y, test_size=0.2, random_state=3255)
)

## Create LightGBM dataset objects
train_data = lgbm.Dataset(X_train, label=y_train)
test_data = lgbm.Dataset(X_test, label=y_test, reference=train_data)


# Begin Modeling

num_round = 100  # Number of boosting rounds
bst = lgbm.train(
    params,
    train_data,
    num_round,
    valid_sets=[test_data],      # Use the test set for validation
)

y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RMSE: {rmse:.2f}')
```

Now, let's run this with hyperparameter selection:

``` {python}
#| echo: false

# Now, ITERATIVE HYPERPARAMETER SELECTION!

## First, parameter grid of options for hyperparameters:
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'min_child_samples': [20, 30, 40],
    'subsample': [0.6, 0.8, 1.0],
}

# Create the LightGBM regressor
lgbm_model = lgbm.LGBMRegressor(objective='regression', metric='rmse')

# Set up GridSearchCV
grid_search = GridSearchCV(
    estimator=lgbm_model,
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error',
    cv=5,  # Number of cross-validation folds
    verbose=1,
    n_jobs=-1  # Use all available cores
)

## Fit GridSearchCV
grid_search.fit(X_train, y_train)

## Check the best parameters and score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best RMSE:", -grid_search.best_score_)

## Train the final model with the best parameters
best_params = grid_search.best_params_
final_model = lgbm.LGBMRegressor(**best_params)
final_model.fit(X_train, y_train)

## Predict and evaluate
y_pred = final_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'Final RMSE: {rmse:.2f}')
```

Now, we have to make predictions for the Travelers test set, and export them in the format provided in order to submit it on Kaggle.

``` {python}
#| echo: false

# Make predictions on the Traveler's test df

predictions = final_model.predict(test_df)

## Create output DataFrame with 'id' and predicted 'call_counts'
output_df = test_df[['id']].copy()  # Copy the 'id' column
output_df['call_counts'] = predictions  # Add predictions

## Output predictions:
output_df.to_csv('data/upgraded_gbm_predictions.csv', index=False)
```

# GLM

