---
title: STAT3255 - Traveler's Case Competition - Modeling
author: Jack Bienvenue
format: html
---

``` {python}
#| echo: false

## Package import
import lightgbm as lgbm
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
import statsmodels.api as sm

## Import clean data
train_df = pd.read_csv('data/clean_training_data.csv')
test_df = pd.read_csv('data/clean_testing_data.csv')
```

Time for the fun part of this competition, that is, doing the modeling!

# Strategy

For any competition, it is important to go in with a strategy. Every competition has its rules, and we can use these to our advantage. On the [Kaggle page](https://www.kaggle.com/competitions/2024-travelers-umc-u-conn/overview), it is mentioned that the *baseline* model, our model to beat, is an untuned LightGBM model:

::: {.callout}
"**Benchmark Model:**

The benchmark will be LightGBM model without any tuning or feature manipulation. We will provide it before the first optional submission."
:::

Given that we know the type of baseline model and we know that it is *untuned*, this presents an opportunity for what should be a surefire beat of the benchmark model if we replicate the baseline method *with* tuning. 

We will try this, and in addition we will consider simpler models. Sometimes simpler models might help us achieve comparable or better results in a much simpler way. We will try out a GLM to do this because we know that the data is heavy-tailed with many entries of $0$ for the call count response.

# LightGBM with Tuning

``` {python}
#| echo: false
#| eval: false

# Initial tuning of parameters, data prep

params = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.1,
    'num_leaves': 31,
    'max_depth': -1,
}

## Set X_train, y_train, X_test, y_test

# NOTE: our *model* train and test set are both from
# just the Kaggle training set (test set does not have)
# entries for call_counts

X = train_df.drop(columns=['call_counts'])
y = train_df['call_counts']

## Split data (80/20)
X_train, X_test, y_train, y_test = (
    train_test_split(X, y, test_size=0.2, random_state=3255)
)

## Create LightGBM dataset objects
train_data = lgbm.Dataset(X_train, label=y_train)
test_data = lgbm.Dataset(X_test, label=y_test, reference=train_data)


# Begin Modeling

num_round = 100  # Number of boosting rounds
bst = lgbm.train(
    params,
    train_data,
    num_round,
    valid_sets=[test_data],      # Use the test set for validation
)

y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'RMSE: {rmse:.2f}')
```

Now, let's run this with hyperparameter selection:

``` {python}
#| echo: false
#| eval: false

# Now, ITERATIVE HYPERPARAMETER SELECTION!

## First, parameter grid of options for hyperparameters:
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'min_child_samples': [20, 30, 40],
    'subsample': [0.6, 0.8, 1.0],
}

# Create the LightGBM regressor
lgbm_model = lgbm.LGBMRegressor(objective='regression', metric='rmse')

# Set up GridSearchCV
grid_search = GridSearchCV(
    estimator=lgbm_model,
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error',
    cv=5,  # Number of cross-validation folds
    verbose=1,
    n_jobs=-1  # Use all available cores
)

## Fit GridSearchCV
grid_search.fit(X_train, y_train)

## Check the best parameters and score
print("Best Hyperparameters:", grid_search.best_params_)
print("Best RMSE:", -grid_search.best_score_)

## Train the final model with the best parameters
best_params = grid_search.best_params_
final_model = lgbm.LGBMRegressor(**best_params)
final_model.fit(X_train, y_train)
```

```{python}

X = train_df.drop(columns=['call_counts'])
y = train_df['call_counts']

## Split data (80/20)
X_train, X_test, y_train, y_test = (
    train_test_split(X, y, test_size=0.2, random_state=3255)
)

best_params = {
    'learning_rate': 0.1,
    'max_depth': -1,
    'min_child_samples': 30,
    'num_leaves': 31,
    'subsample': 0.6
}

final_model = lgbm.LGBMRegressor(**best_params)
final_model.fit(X_train, y_train)

## Predict and evaluate
y_pred = final_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'Final RMSE: {rmse:.2f}')
```

Now, we have to make predictions for the Travelers test set, and export them in the format provided in order to submit it on Kaggle.

``` {python}
#| echo: false

# Make predictions on the Traveler's test df

predictions = final_model.predict(test_df)

## Create output DataFrame with 'id' and predicted 'call_counts'
output_df = test_df[['id']].copy()  # Copy the 'id' column
output_df['Predict'] = predictions  # Add predictions

## Output predictions:
output_df.to_csv('predictions/new_gbm_predictions.csv', index=False)
```


Output from optimization trial:

[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005956 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 1247
[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 33
[LightGBM] [Info] Start training from score 25.844641
Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 30, 'num_leaves': 31, 'subsample': 0.6}
Best RMSE: 36.09774244904193
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005309 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 1247
[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 33
[LightGBM] [Info] Start training from score 25.844641
Final RMSE: 35.93

# GLM (Poisson)

``` {python}
# Define the features and target
X = train_df.drop(columns=['call_counts', 'newest_veh_age'])  # Drop both target and unwanted column
y = train_df['call_counts']

# Drop rows with missing values in 'pol_edeliv_ind' and 'telematics_ind'
X = X.dropna(subset=['pol_edeliv_ind', 'telematics_ind'])

# Align y with the remaining rows in X
y = y[X.index]

# Add a constant for the intercept
X = sm.add_constant(X)
```

```{python}
test_df = sm.add_constant(test_df, has_constant='add')
test_df = test_df[X.columns]

# Define the probabilities and corresponding values
pol_edeliv_values = [0.0, 1.0]
pol_edeliv_probs = [0.540088, 0.459912]

telematics_values = [-2.0, -1.0, 1.0]
telematics_probs = [0.798611, 0.102403, 0.098986]

# Assign missing values for 'pol_edeliv_ind' based on the defined probabilities
missing_pol_edeliv = test_df['pol_edeliv_ind'].isnull()
test_df.loc[missing_pol_edeliv, 'pol_edeliv_ind'] = np.random.choice(
    pol_edeliv_values, 
    size=missing_pol_edeliv.sum(), 
    p=pol_edeliv_probs
)

# Assign missing values for 'telematics_ind' based on the defined probabilities
missing_telematics = test_df['telematics_ind'].isnull()
test_df.loc[missing_telematics, 'telematics_ind'] = np.random.choice(
    telematics_values, 
    size=missing_telematics.sum(), 
    p=telematics_probs
)

print("Missing values in X:\n", X.isnull().sum())
print("Missing values in y:", y.isnull().sum())
print("Missing values in test:", test_df.isnull().sum())

boolean_columns = ['bi_limit_group_CSL', 'bi_limit_group_NonAuto', 'bi_limit_group_SP']  # Update this list with your actual boolean columns

X[boolean_columns] = X[boolean_columns].apply(lambda x: x.astype(int))
test_df[boolean_columns] = test_df[boolean_columns].apply(lambda x: x.astype(int))

print(X.dtypes)
print(test_df.dtypes)


# Fit the model
poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()

print(poisson_model.summary())

predictions = poisson_model.predict(test_df)

## Create output DataFrame with 'id' and predicted 'call_counts'
output_df = test_df[['id']].copy()  # Copy the 'id' column
output_df['Predict'] = predictions  # Add predictions

print(output_df)

## Output predictions:
output_df.to_csv('predictions/poisson_predictions.csv', index=False)
```

``` {python}
# Penalty model:

## First 60% of predictions -> 0
## Last 40%, scale 

```

``` {python}
#| eval: false

def relative_gini(actual, pred):
    # Sort by predicted values, keeping track of actual values
    sorted_indices = np.argsort(pred)
    sorted_actual = np.array(actual)[sorted_indices]
    
    # Cumulative sum of actual values and a baseline
    cum_actual = np.cumsum(sorted_actual)
    cum_index = np.cumsum(np.ones_like(sorted_actual))
    
    # Compute Gini for predictions
    gini_pred = (np.sum(cum_actual / cum_actual[-1]) - (cum_index[-1] + 1) / 2) / cum_index[-1]
    
    # Compute Gini for perfect model (actual values sorted by themselves)
    sorted_actual_perfect = np.sort(actual)
    cum_actual_perfect = np.cumsum(sorted_actual_perfect)
    gini_actual = (np.sum(cum_actual_perfect / cum_actual_perfect[-1]) - (cum_index[-1] + 1) / 2) / cum_index[-1]
    
    # Return the relative Gini coefficient
    return gini_pred / gini_actual if gini_actual != 0 else 0
```